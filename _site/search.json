[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting House Prices with Machine Learning\n\n\n\nPython\n\n\nMachine Learning\n\n\nData Cleaning\n\n\n\nThis project involves using machine learning algorithms to predict house prices based on various features such as location, size, and amenities. It includes data cleaning, feature engineering, and model selection.\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segmentation Using Clustering Techniques\n\n\n\nR\n\n\nMachine Learning\n\n\nClustering\n\n\nStatistical Modelling\n\n\n\nThis project focuses on segmenting customers into different groups based on their purchasing behavior and demographics. It uses clustering algorithms like K-means and hierarchical clustering to identify distinct customer segments.\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Global CO2 Emissions\n\n\n\nR\n\n\nData Visualization\n\n\nEnvironmental Science\n\n\n\nThis project involves creating visualizations to show trends in global CO2 emissions over time. It includes data extraction from public databases, data cleaning, and using visualization libraries to create interactive charts and graphs.\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "FinalProject.html",
    "href": "FinalProject.html",
    "title": "Shake It Off-ense: A Statitistical Study On Taylor Swifts Influence On The Kansas City Chiefs",
    "section": "",
    "text": "Over the past 3 years I’ve become interested in fantasy football. This has lead me to explore football statistics in order to better predict the performance of players and teams. I’ve also grown up with two sisters who have been die hard Taylor Swift fans their whole lives. Though these two facts about my life seem to be unrelated however, If you’ve watched a Kansis city chiefs game in the past two years or heard any news at all about Taylor Swift, you would know that they are unfortunately very related. The world was set ablaze in September 2023 when Taylor Swift and Travis Kelce announced they were in a relationship. Ticket and Travis Kelce Jersey revenue skyrocketed as the market was introduced to millions of Swifties trying to secure the latest Taylor Swift clout so they could where it to her next show. As their high profile relationship continues, I am curious to see how this relationship would effect the Kansas City Chiefs Performance."
  },
  {
    "objectID": "FinalProject.html#motivation-and-context",
    "href": "FinalProject.html#motivation-and-context",
    "title": "Shake It Off-ense: A Statitistical Study On Taylor Swifts Influence On The Kansas City Chiefs",
    "section": "",
    "text": "Over the past 3 years I’ve become interested in fantasy football. This has lead me to explore football statistics in order to better predict the performance of players and teams. I’ve also grown up with two sisters who have been die hard Taylor Swift fans their whole lives. Though these two facts about my life seem to be unrelated however, If you’ve watched a Kansis city chiefs game in the past two years or heard any news at all about Taylor Swift, you would know that they are unfortunately very related. The world was set ablaze in September 2023 when Taylor Swift and Travis Kelce announced they were in a relationship. Ticket and Travis Kelce Jersey revenue skyrocketed as the market was introduced to millions of Swifties trying to secure the latest Taylor Swift clout so they could where it to her next show. As their high profile relationship continues, I am curious to see how this relationship would effect the Kansas City Chiefs Performance."
  },
  {
    "objectID": "FinalProject.html#main-objective",
    "href": "FinalProject.html#main-objective",
    "title": "Shake It Off-ense: A Statitistical Study On Taylor Swifts Influence On The Kansas City Chiefs",
    "section": "Main Objective",
    "text": "Main Objective\nHow does Taylor Swift’s attendance at Kansas City Chiefs games effect the offensive performance."
  },
  {
    "objectID": "FinalProject.html#packages-used-in-this-analysis",
    "href": "FinalProject.html#packages-used-in-this-analysis",
    "title": "Shake It Off-ense: A Statitistical Study On Taylor Swifts Influence On The Kansas City Chiefs",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nlibrary(splines)\nlibrary(tidyverse)\nlibrary(kknn)\nlibrary(Lahman)\nlibrary(probably)\nlibrary(knockoff)\nlibrary(selectiveInference)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(readr)\nlibrary(stringr)\nlibrary(ggplot2)  \nlibrary(dplyr)\n\n\n\n\nPackage\nUse\n\n\n\n\nhere\nto easily load and save data\n\n\nreadr\nto import the CSV file data\n\n\ndplyr\nto massage and summarize data\n\n\nrsample\nto split data into training and test sets\n\n\nggplot2\nto create nice-looking and informative graphs\n\n\ntidyverse\na collection of R packages designed for data science\n\n\nkknn\nfor k-nearest neighbors classification and regression\n\n\nLahman\nprovides baseball statistics data for analysis\n\n\nprobably\nto post-process and threshold model predictions\n\n\nknockoff\nfor feature selection with controlled false discovery rate\n\n\nselectiveInference\nfor valid statistical inference after model selection\n\n\ntidymodels\na unified framework for modeling and machine learning\n\n\nmodeltime\nfor time series forecasting using tidymodels workflow\n\n\nstringr\nto manipulate and work with strings easily"
  },
  {
    "objectID": "FinalProject.html#data-description",
    "href": "FinalProject.html#data-description",
    "title": "Shake It Off-ense: A Statitistical Study On Taylor Swifts Influence On The Kansas City Chiefs",
    "section": "Data Description",
    "text": "Data Description\nThe data I will be using is Kansas City Chiefs season performance statistics form https://www.pro-football-reference.com/ from 2013 to 2024. This data is collected from the staff of Pro Football Reference. This site is the leading football statistical reference site, and is sited by many others. I am constricting data to 2013 - 2024 since 2013 is when Patrick Mahomes was drafted onto the Kansas City Chiefs ans started a new era of their franchise.\nVariables\nYear - Year game took place. Week - Week of the season game took place. Day - Day of the week game took place. Time - Time of day game took place. Outcome - Outcome of game. OT - Indicates overtime took place. Rec - Redord of team. Location - Indicates hom or away game. Ppp - Opponent team name. PtS - Points scored. PtA - Points Allowed. FirstD - First down count. TotYd - Total yards gained. PassY - Passing yards. RushY - Rushng yards. TO - Timeouts taken. FirstD_Alwd - Opponentfirst dow count. TotYd_Alwd - Opponent total yards gained. PassY_Alwd - Opponent total passing yards. RushY_Alwd - Opponent total rushing yards. TO_Alwd - Opponent touchdowns ExOff - Expected Offensive performance. ExDef - Average expected points for every play. ExSpec - Expected special teams performance. Taylor_Attend - Indicates attendance of Taylor Swift\n\nkcc_g_16 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_16.txt\", show_col_types = FALSE)\nkcc_g_24 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_24.txt\", show_col_types = FALSE)\nkcc_g_23 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_23.txt\", show_col_types = FALSE)\nkcc_g_22 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_22.txt\", show_col_types = FALSE)\nkcc_g_21 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_21.txt\", show_col_types = FALSE)\nkcc_g_20 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_20.txt\", show_col_types = FALSE)\nkcc_g_19 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_19.txt\", show_col_types = FALSE)\nkcc_g_18 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_18.txt\", show_col_types = FALSE)\nkcc_g_17 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_17.txt\", show_col_types = FALSE)\nkcc_g_16 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_16.txt\", show_col_types = FALSE)\nkcc_g_15 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_15.txt\", show_col_types = FALSE)\nkcc_g_14 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_14.txt\", show_col_types = FALSE)\nkcc_g_13 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_13.txt\", show_col_types = FALSE)\n\n\nData Limitations\nChoosing a variable to indicate the success of the offense is the first step. Variables such as Outcome, PtS (points scored), TotYd (total yards gained) all seem to be adequate indicators for success for an offence. It is significant that we are measuring strictly offensive performance. Therefore any variable also indicating Defensive performance or Special teams performance should be left out. Whn using Outcome as an indicator fo success, we’d be studying whether the Chiefs won or lost a game. In football , the defensive performance is largly viewed as more important than offensive performance iin securing a win. For this reason, whether a team won or loss is largly an indcator whether the defence performed well or not. For this reason we will not use Outcome as a response variable. TotYd (total yards gained) seems to be a good response variable because it shows how many yards the offense was able to gain during a game. This does not include yards gained by defense or special teams. It makes sense to explore yards gained by an offense to measure success, however the goal of an offense is not to merely gain yards, but to score points. This variable fails to reward offenses for scoring points. The advantage to using PtS (points scored) to measure for success is that it rewards offenses for yards gained and for scoring points. It also rewards offenses for field goals which indicates a mild success of an offense. However, this variable indicates total points gained by offense, defense and special teams. THis means that defenseive scores i.e. pick sixes, fumble recoveries for touchdowns and safeties, also contributre to this variable. Extra points also contribute to this data. This does points score less accurate when strickly measuring offensive performance, however these forms of scoring not associated with the offense are rare and will not effect our results significantly. The perfect variable we could use would be offensive points scored, however this variable does not exist, therefore we will settle with Points scored.\nAnother important factor when measuring offensive success is the skill of the opposing defense. Points scored against some teams are more impressive than other teams. 30 points scored against an inadequate defense may be less impressive that 14 points scored against the top defense in the league. To account for this, a variable could be constructed indicating the difference between predicted verse actual points scred by offene. THis variable would then indicate how well an offense performed against induvidual defenses. However, predicted points scored by offense is not a variable we have access to.\nWe will proceed to use PtS (points scored) as a response variable for the success of the Chiefs Offense. The drawbacks and inaccuracies that some with using this variable as a indicator for offensive success should be kept in mind when considering the acccuracy of our model."
  },
  {
    "objectID": "FinalProject.html#data-wrangling",
    "href": "FinalProject.html#data-wrangling",
    "title": "Shake It Off-ense: A Statitistical Study On Taylor Swifts Influence On The Kansas City Chiefs",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nSome data wrangling for our data sets to be in a usable format. We will move the column and row names to appropriate positions in our data frame. We will also rmeove week title and substitue them, for week numbers. We will also add a Taylor Attendance variable from data produced by me.\n\nfile_paths &lt;- list.files(path = \"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data\", \n                         pattern = \"kcc_g_.*\\\\.txt$\", full.names = TRUE)\n\nnew_col_names &lt;- c(\"Week\", \"Day\", \"Date\", \"Time\", \"Boxscore\", \"Outcome\", \"OT\", \"Rec\", \n                   \"Location\", \"Ppp\", \"PtS\", \"PtA\", \"FirstD\", \"TotYd\", \"PassY\", \"RushY\", \n                   \"TO\", \"FirstD_Alwd\", \"TotYd_Alwd\", \"PassY_Alwd\", \"RushY_Alwd\", \n                   \"TO_Alwd\", \"ExOff\", \"ExDef\", \"ExSpec\")\n\nkcc_data &lt;- lapply(file_paths, function(file) {\n  df &lt;- read_csv(file, skip = 1, show_col_types = FALSE)   # Read the file\n  colnames(df) &lt;- new_col_names  # Rename columns\n  year_suffix &lt;- str_extract(basename(file), \"\\\\d{2}\")\n  full_year &lt;- as.integer(paste0(\"20\", year_suffix))  # Convert to 4-digit year\n\n  df &lt;- df |&gt; filter(Week != \"Playoffs\") #Removes playoff row\n  df &lt;- df |&gt; filter(Ppp != \"Bye Week\") #removes bie week row\n  df &lt;- df |&gt; mutate(Taylor_Attend = \"no\") #Adds attendance collumn\n  df &lt;- df |&gt;  mutate(Year = full_year) |&gt;  # Add Year column\n    \n    relocate(Year, .before = Week)  # Move Year column before Week\n  df &lt;- df |&gt; mutate(\n    Date = paste(Date, full_year),  # Combine Date and Year\n    Date = as.Date(Date, format = \"% %d %Y\")  # Convert to Date format (e.g., \"September 13 2013\")\n  )\n\n  df &lt;- df |&gt; mutate(\n    Week = case_when(  # Replace playoff names with sequential week numbers\n        Week == \"Wild Card\" ~ \"18\",\n        Week == \"Division\" ~ \"19\",\n        Week == \"Conf. Champ.\" ~ \"20\",\n        Week == \"SuperBowl\" ~ \"21\",\n        TRUE ~ as.character(Week)  # Keep regular weeks unchanged\n      )\n  )\n  return(df)  # Return the modified dataframe\n})\n\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...9`\n• `Opp` -&gt; `Opp...10`\n• `Opp` -&gt; `Opp...12`\n• `1stD` -&gt; `1stD...13`\n• `TotYd` -&gt; `TotYd...14`\n• `PassY` -&gt; `PassY...15`\n• `RushY` -&gt; `RushY...16`\n• `TO` -&gt; `TO...17`\n• `1stD` -&gt; `1stD...18`\n• `TotYd` -&gt; `TotYd...19`\n• `PassY` -&gt; `PassY...20`\n• `RushY` -&gt; `RushY...21`\n• `TO` -&gt; `TO...22`\n\nnames(kcc_data) &lt;- gsub(\".*/|\\\\.txt$\", \"\", file_paths)  # Remove path and .txt extension\n\n# Assign each dataframe to a separate variable\nfor (name in names(kcc_data)) {\n  assign(name, kcc_data[[name]])\n}\n\nTaylor_Attend23 &lt;- c(\"no\", \"no\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\")\nTaylor_Attend24 &lt;- c(\"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\")\n\nkcc_g_23 &lt;- kcc_g_23 |&gt; mutate(Taylor_Attend = Taylor_Attend23)\nkcc_g_24 &lt;- kcc_g_24 |&gt; mutate(Taylor_Attend = Taylor_Attend24)"
  },
  {
    "objectID": "FinalProject.html#exploratory-data-analysis",
    "href": "FinalProject.html#exploratory-data-analysis",
    "title": "Shake It Off-ense: A Statitistical Study On Taylor Swifts Influence On The Kansas City Chiefs",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFor our EDA we will look the Chiefs performance during Travis Kelce and Taylor Swift’s relationship. Ther relationship began in Septermber of 2023 with Taylporo attending her frst Chiefs game on September 24th, 2023. For this reason, we will look at the Chiefs 2023 and 2024 seasons to access our data.\nWhen creating a training set for our data, we will go back to 2013. This was the year Andy Reid became the head coach of the Kansas City Chiefs and marks the beginning of a new era for the team. We are not using data from before 2013, as it would skew our model toward the performance of the Chiefs during a fundamentally different period — before Reid’s leadership and the offensive system he brought with him. Including older data would dilute the model’s ability to learn patterns that reflect the team’s modern strategies, play style, and overall performance under Reid’s coaching.\n\nyears_to_include &lt;- list(kcc_g_13, kcc_g_14, kcc_g_15, kcc_g_16, kcc_g_17, kcc_g_18, kcc_g_19, kcc_g_20, kcc_g_21, kcc_g_22)\n\nkcc_g_train &lt;- bind_rows(years_to_include)\n\nkcc_g_test &lt;- bind_rows(kcc_g_23, kcc_g_24)\n\nkcc_g_all &lt;- bind_rows(kcc_g_train, kcc_g_test)\n\nkcc_g_dyn &lt;- bind_rows(kcc_g_19, kcc_g_20, kcc_g_21, kcc_g_22, kcc_g_23, kcc_g_24)\n\nkcc_g_taylor &lt;- filter(kcc_g_test, Taylor_Attend == \"yes\")\n\nkcc_g_taylorless &lt;- filter(kcc_g_test, Taylor_Attend == \"no\")\n\nTo give context as to how the Chiefs performed during his time period, here is the distribution of points scored for 2013 to 2024 seasons.\n\nggplot(kcc_g_all, aes(x = as.factor(Year), y = PtS, fill = as.factor(Year))) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Points Scored Per Game Distribution by Year\",\n       x = \"Year\", \n       y = \"Points Scored Per Game (PtS)\")\n\n\n\n\n\n\n\n\nA notable increase in points scored can be observed from 2017 to 2018. This was the year that Patrick Mahomes became the starting quarterback. It may be necessary to constrict our training data to 2018 - 2024 data, however this would lead to far less data to train our model with (only 4 seasons of data!). Therefore we will proceed with or initial 2013 to 2024 seasons.\nWe will now explore the distribution of Points scored by the Chiefs when Taylor Attended verses games she did not attend.\n\nkcc_g_test |&gt; group_by(Taylor_Attend) |&gt; \n  summarize(\n    num_total = n(),\n    num_missing = sum(is.na(PtS)),\n    min = min(PtS, na.rm = TRUE),\n    Q1 = quantile(PtS, 0.25, na.rm = TRUE),\n    median = median(PtS, na.rm = TRUE),\n    Q3 = quantile(PtS, 0.75, na.rm = TRUE),\n    max = max(PtS, na.rm = TRUE),\n    mean = mean(PtS, na.rm = TRUE),\n    sd = sd(PtS, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 10\n  Taylor_Attend num_total num_missing   min    Q1 median    Q3   max  mean    sd\n  &lt;chr&gt;             &lt;int&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 no                   18           0     0    17     21    27    31  20.5  7.91\n2 yes                  23           0    14    19     25    27    41  24.3  6.14\n\nggplot(data = kcc_g_test, aes(x = Taylor_Attend, y = PtS, fill = Taylor_Attend)) + \n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Avoids overlapping points\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Boxplot of PtS during 2023 - 2024\",\n       x = \"Taylor Attend\",\n       y = \"PtS\") +\n  scale_fill_manual(values = c( \"no\" = \"red\", \"yes\" = \"blue\"), labels = c(\"Games Taylor Did Not Attend\", \"Games Taylor Attended\"))\n\n\n\n\n\n\n\n\nAs you can see, the games which Taylor attended seem to have a greater distribution than the games she did not attend with about a 4 point greater mean. We will also note that the two highest scoring games occurred when Taylor was attending and the two lowest scoring games occurred when she was not attending.\nWe will proceed to observe the relationship our variables have with Points Scored.\nThe following is a plot of Total Yards and Points Scored, with blue and red data points indicating Taylor’s attendance and non-attendance, respectively.\n\nggplot(kcc_g_test, aes(x = TotYd, y = PtS, color = Taylor_Attend)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(\n    title = \"Chiefs Points Scored vs Total Yards\",\n    x = \"Total Yards (TotYd)\",\n    y = \"Points Scored (PtS)\",\n    color = \"Taylor Swift Attendance\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnsurprisingly, we can observe a fairly linear relationship here.\nNext we will observe the relationship between ExOff and PtS variables. Unfortunately, I am unaware as to what ExOff is predicting. It is an indication of some sorts of the offensive performance of the team. I am hypothesizing that it is the average expected score for each offensive play of the game.\n\nggplot(kcc_g_test, aes(x = ExOff, y = PtS, color = Taylor_Attend)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(\n    title = \"Chiefs ExOff vs Total Yards\",\n    x = \"Total Yards (TotYd)\",\n    y = \"Points Scored (PtS)\",\n    color = \"Taylor Swift Attendance\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nA linear relationship is again observable; however, we have chosen not to include this variable in our model, as its underlying significance is unclear.\nLastly, we will observe the relationship between fist down count and Points scored.\n\nlibrary(ggplot2)\n\nggplot(kcc_g_test, aes(x = FirstD, y = PtS, color = Taylor_Attend)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(\n    title = \"Chiefs First Down Count vs Total Yards\",\n    x = \"Total Yards (TotYd)\",\n    y = \"Points Scored (PtS)\",\n    color = \"Taylor Swift Attendance\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe will now find appropriate explanatory variables with a LASSO model\n\nkcc_lasso_model &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), \n                          mixture = 1) \n\nkcc_lasso_recipe &lt;- recipe(\n  PtS ~ PtA \n  + FirstD \n  + TotYd \n  + PassY \n  + RushY \n  + TotYd_Alwd \n  + PassY_Alwd \n  + RushY_Alwd, \n  data = kcc_g_train\n) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; \n  step_dummy(all_nominal_predictors())\n\nkcc_lasso_wflow &lt;- workflow() |&gt;\n  add_model(kcc_lasso_model) |&gt;\n  add_recipe(kcc_lasso_recipe) \n\n\nkcc_cv &lt;- vfold_cv(kcc_g_train, v = 10)\nkcc_lasso_tune &lt;- tune_grid(kcc_lasso_model, \n                      kcc_lasso_recipe, \n                      resamples = kcc_cv, \n                      grid = grid_regular(penalty(range = c(-2, 2)), levels = 50))\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\n\n\nkcc_lasso_tune |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nlasso_best &lt;- kcc_lasso_tune |&gt;\n  select_best(\n    metric = \"rmse\"\n)\nlasso_best\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.295 Preprocessor1_Model19\n\n\nWe will do a calibration check to see how our penalized variable do in predicting our model.\n\nlibrary(modeltime)\nlibrary(ggplot2)\nlasso_wflow_final &lt;- kcc_lasso_wflow |&gt;\n  finalize_workflow(parameters = lasso_best) \n\nlasso_pred_check &lt;- lasso_wflow_final |&gt;\n  fit_resamples(\n    resamples = kcc_cv,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\ncal_plot_regression(\n  lasso_pred_check,\n  truth = PtS,\n  estimate = .pred\n)\n\n\n\n\n\n\n\n\nThe Lasso model does mildly well in predicting values.\n\nkcc_lasso_fit &lt;- lasso_wflow_final |&gt;\n  fit(data = kcc_g_train)\nkcc_lasso_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00 4.9620\n2   1  4.72 4.5210\n3   1  8.63 4.1190\n4   1 11.88 3.7530\n5   1 14.58 3.4200\n6   1 16.82 3.1160\n7   1 18.68 2.8390\n8   1 20.22 2.5870\n9   1 21.51 2.3570\n10  1 22.57 2.1480\n11  1 23.45 1.9570\n12  1 24.19 1.7830\n13  2 25.10 1.6250\n14  3 26.07 1.4800\n15  3 26.88 1.3490\n16  4 27.56 1.2290\n17  4 28.27 1.1200\n18  4 28.87 1.0200\n19  4 29.36 0.9297\n20  4 29.76 0.8471\n21  4 30.10 0.7719\n22  4 30.38 0.7033\n23  4 30.62 0.6408\n24  4 30.81 0.5839\n25  4 30.97 0.5320\n26  4 31.10 0.4848\n27  4 31.21 0.4417\n28  4 31.31 0.4025\n29  4 31.38 0.3667\n30  4 31.45 0.3341\n31  4 31.50 0.3044\n32  4 31.54 0.2774\n33  4 31.58 0.2528\n34  4 31.61 0.2303\n35  4 31.63 0.2098\n36  4 31.65 0.1912\n37  4 31.67 0.1742\n38  4 31.69 0.1587\n39  5 31.71 0.1446\n40  6 31.76 0.1318\n41  6 31.80 0.1201\n42  6 31.83 0.1094\n43  6 31.86 0.0997\n44  6 31.88 0.0908\n45  6 31.90 0.0828\n46  6 31.92 0.0754\n\n...\nand 22 more lines.\n\n\n\nkcc_lasso_fit |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nkcc_lasso_coef &lt;- kcc_lasso_fit |&gt;\n  broom::tidy()\nkcc_lasso_coef \n\n# A tibble: 9 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   27.8     0.295\n2 PtA            0       0.295\n3 FirstD         0.704   0.295\n4 TotYd          3.72    0.295\n5 PassY          0       0.295\n6 RushY          0.730   0.295\n7 TotYd_Alwd     0       0.295\n8 PassY_Alwd     1.35    0.295\n9 RushY_Alwd     0       0.295\n\n\nHere we can see that important variables are, First Down Count, Total Yards, Rushing Yards, and Passing Yards Allowed. We will now train a model with this data."
  },
  {
    "objectID": "FinalProject.html#modeling",
    "href": "FinalProject.html#modeling",
    "title": "Shake It Off-ense: A Statitistical Study On Taylor Swifts Influence On The Kansas City Chiefs",
    "section": "Modeling",
    "text": "Modeling\n\nModel Selection\nWe will start by choosing an appropriate model. The models we will be considering are: Linear Regression, Polynomial Regression, K-Nearest Neighbors, and Spline Regression (Natural Splines). We will compare RMSE or Root Mean Squared Errors of these models to determine which model fits our data best. Root Mean Squared Error is defined as:\n\\[\n\\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2 }\n\\]\nRMSE measures how off a given model is by finding the average squared difference between predicted values and actual values. We then take the square root of this value to account for the square that occurred in the residual terms. A higher value RMSE indicates on average, a higher amount of inaccuracy in a model. Therefore, we will be looking for the model with the lowest RSME.\nIn order to determine the RMSE of these models, we will use 10-fold Cross Validation (10-fold CV). The process of 10-fold CV involves splitting our training datainto 10 equal parts or “folds”. We then take these folds in tur and hold them out as test sets. We train our moel on te remaining 9 folds, then test our model on the remaining heldout test fold, and find the RMSE. After repeating this process with the 9 other folds we will have 10 RMSE values tat we will take the averae of. We will do thiisfor each model then determine whic model produces the smallest RMSE.\n\nkcc_linreg_model &lt;- linear_reg(mode = \"regression\", engine = \"lm\")\nkcc_knn_model &lt;- nearest_neighbor(mode = \"regression\",\n                              engine = \"kknn\",\n                              neighbors = tune(), dist_power = 2)\n\n\nkcc_linear_recipe &lt;- recipe(\n  PtS ~ FirstD\n  + TotYd \n  + RushY \n  + PassY_Alwd, \n  data = kcc_g_train\n)\n\nkcc_quadratic_recipe &lt;- recipe(\n  PtS ~ FirstD\n  + TotYd \n  + RushY \n  + PassY_Alwd,\n  data = kcc_g_train\n) |&gt;\n  step_poly(TotYd, degree = 2)\n\nkcc_knn_recipe &lt;- recipe(\n  PtS ~ FirstD\n  + TotYd \n  + RushY \n  + PassY_Alwd,\n  data = kcc_g_train\n) |&gt;\n  step_normalize(all_numeric_predictors())\n\nkcc_ns_recipe &lt;- recipe( PtS ~ FirstD\n  + TotYd \n  + RushY \n  + PassY_Alwd,\n  data = kcc_g_train) |&gt; step_ns(TotYd, deg_free = 4)\n\n\nall_models &lt;- workflow_set(\n  preproc = list(linear = kcc_linear_recipe, quadratic = kcc_quadratic_recipe, knn = kcc_knn_recipe, kcc_ns_recipe),\n  models = list(lr = kcc_linreg_model,lr = kcc_linreg_model, knn = kcc_knn_model, lr = kcc_linreg_model),\n  cross = FALSE # don't mix knn recipes with linear models or vice-versa\n)\nall_models\n\n# A workflow set/tibble: 4 × 4\n  wflow_id     info             option    result    \n  &lt;chr&gt;        &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 linear_lr    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 quadratic_lr &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 knn_knn      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_4_lr  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\n\n# 10-fold cv, not repeated\nset.seed(1112)\nkcc_cv &lt;- vfold_cv(kcc_g_train, v = 10)\n\n\nknn.grid &lt;- expand.grid(neighbors = seq(2,16, by = 2)) # set up tuning grid\n\nall_models &lt;- workflow_set(\n  preproc = list(linear = kcc_linear_recipe,\n                 quadratic = kcc_quadratic_recipe,\n                 knn = kcc_knn_recipe,\n                 ns = kcc_ns_recipe\n                 ),\n  models = list(lr = kcc_linreg_model,\n                lr = kcc_linreg_model,\n                knn2 = kcc_knn_model,\n                lr = kcc_linreg_model), # this is what is being tuned\n  cross = FALSE \n)\n\nall_models &lt;- all_models |&gt;\n  # add the grid for JUST the knn model\n  option_add(grid = knn.grid, id = \"knn_knn2\") |&gt; #adding grid\n  workflow_map(\"tune_grid\",\n               resamples = kcc_cv,\n               metrics = metric_set(rmse), # can add more\n               verbose = TRUE)\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 1 of 4 resampling: linear_lr\n\n\n✔ 1 of 4 resampling: linear_lr (1.4s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 2 of 4 resampling: quadratic_lr\n\n\n✔ 2 of 4 resampling: quadratic_lr (2.1s)\n\n\ni 3 of 4 tuning:     knn_knn2\n\n\n✔ 3 of 4 tuning:     knn_knn2 (3.8s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 4 of 4 resampling: ns_lr\n\n\n✔ 4 of 4 resampling: ns_lr (1.5s)\n\nall_models\n\n# A workflow set/tibble: 4 × 4\n  wflow_id     info             option    result   \n  &lt;chr&gt;        &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 linear_lr    &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n2 quadratic_lr &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n3 knn_knn2     &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n4 ns_lr        &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n\n\nWe will now plot the RMSE values to compare values.\n\nautoplot(all_models)\n\n\n\n\n\n\n\n\nThe 3 red RMSE values are the linear, quadratic and spline regression models. The blue RMSE values are the K-Nearest Neighbor values with different K-values. As you can see the linear, quadratic and spline regression models preform better than the K-nearest Neighbors model. We will now determine the RMSE values of our linear, quadratic and spline regression models.\n\nrank_results(all_models) |&gt;\n  dplyr::select(wflow_id, .config, .metric, mean, std_err, rank) |&gt;\n  arrange(.metric, rank)\n\n# A tibble: 11 × 6\n   wflow_id     .config              .metric  mean std_err  rank\n   &lt;chr&gt;        &lt;chr&gt;                &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n 1 quadratic_lr Preprocessor1_Model1 rmse     7.77   0.479     1\n 2 linear_lr    Preprocessor1_Model1 rmse     7.81   0.471     2\n 3 ns_lr        Preprocessor1_Model1 rmse     7.85   0.480     3\n 4 knn_knn2     Preprocessor1_Model8 rmse     7.93   0.497     4\n 5 knn_knn2     Preprocessor1_Model7 rmse     7.95   0.492     5\n 6 knn_knn2     Preprocessor1_Model6 rmse     8.00   0.493     6\n 7 knn_knn2     Preprocessor1_Model5 rmse     8.07   0.497     7\n 8 knn_knn2     Preprocessor1_Model4 rmse     8.16   0.496     8\n 9 knn_knn2     Preprocessor1_Model3 rmse     8.33   0.507     9\n10 knn_knn2     Preprocessor1_Model2 rmse     8.61   0.506    10\n11 knn_knn2     Preprocessor1_Model1 rmse     9.42   0.548    11\n\n\nThe quadratic model performed best with an RMSE of 7.77. We will proceed to use this to model our training set."
  },
  {
    "objectID": "FinalProject.html#insights",
    "href": "FinalProject.html#insights",
    "title": "Shake It Off-ense: A Statitistical Study On Taylor Swifts Influence On The Kansas City Chiefs",
    "section": "Insights",
    "text": "Insights\nOur Taylor-attended model has a bias of 1.47, while the Taylor-less model has a bias of 2.97 with a difference of 1.500. Bias is calculated from the perspective of the model, meaning that a positive bias indicates the model is over predicting, while a negative bias would indicate under prediction. Smaller absolute values of bias suggest a better model fit. The larger positive bias in the Taylor-less model suggests that the model over predicted more frequently when Taylor Swift did not attend games.\nSince both models have positive bias values, this implies that the Chiefs tended to under perform in both scenarios. However, the higher bias in games without Taylor’s attendance indicates a greater average under performance in those games, relative to what the model predicted. This is an indication that the Chiefs tend to perform slightly better when Taylor Swift is present at their games.\nThe average points score of Chiefs games from 2013 to 2024 is 26.85. This would mean that if Taylor’s attendance would theoretically have a 5.58% increase in score. This is a fairly significant advantage when observing Points scored as an indicator of offensive success. However, between 2013 and 2024, there are only 10 games where the Chiefs lost by 2 points or fewer—scenarios where a 1.5-point scoring advantage could conceivably have changed the outcome of the game.\n\nLimitations and Future Work\n\n\nReflection (Optional Subsection)"
  },
  {
    "objectID": "blog/second-post/index.html",
    "href": "blog/second-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Amet cursus sit amet dictum sit amet. Eget duis at tellus at urna condimentum. Convallis aenean et tortor at risus viverra. Tincidunt ornare massa eget egestas purus viverra accumsan. Et malesuada fames ac turpis egestas. At imperdiet dui accumsan sit amet. Ut ornare lectus sit amet est placerat. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Duis ultricies lacus sed turpis tincidunt id aliquet risus. Mattis enim ut tellus elementum sagittis. Dui id ornare arcu odio ut. Natoque penatibus et magnis dis. Libero justo laoreet sit amet cursus sit. Sed faucibus turpis in eu. Tempus iaculis urna id volutpat lacus laoreet.\nPhasellus vestibulum lorem sed risus. Eget felis eget nunc lobortis mattis. Sit amet aliquam id diam maecenas ultricies. Egestas maecenas pharetra convallis posuere morbi. Etiam erat velit scelerisque in dictum non consectetur a erat. Cras fermentum odio eu feugiat pretium nibh ipsum consequat. Viverra accumsan in nisl nisi scelerisque. Et netus et malesuada fames ac. Amet tellus cras adipiscing enim eu turpis egestas pretium aenean. Eget lorem dolor sed viverra ipsum nunc aliquet. Ultrices dui sapien eget mi proin sed libero enim sed. Ultricies mi eget mauris pharetra et ultrices neque. Ipsum suspendisse ultrices gravida dictum. A arcu cursus vitae congue mauris rhoncus aenean vel. Gravida arcu ac tortor dignissim convallis. Nulla posuere sollicitudin aliquam ultrices."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/first-post/index.html",
    "href": "blog/first-post/index.html",
    "title": "First Post",
    "section": "",
    "text": "Sed risus ultricies tristique nulla aliquet. Neque volutpat ac tincidunt vitae semper quis lectus nulla.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Enim sed faucibus turpis in eu mi bibendum neque. Ac orci phasellus egestas tellus rutrum tellus pellentesque eu. Velit sed ullamcorper morbi tincidunt ornare massa. Sagittis id consectetur purus ut faucibus pulvinar elementum integer. Tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Lobortis feugiat vivamus at augue eget arcu. Aliquam ut porttitor leo a diam sollicitudin tempor id eu. Mauris a diam maecenas sed enim ut sem viverra aliquet. Enim ut tellus elementum sagittis vitae et leo duis. Molestie at elementum eu facilisis sed odio morbi quis commodo. Sapien pellentesque habitant morbi tristique senectus. Quam vulputate dignissim suspendisse in est. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.\nVelit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Viverra mauris in aliquam sem fringilla ut morbi tincidunt augue. Tortor at auctor urna nunc id. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Aliquet nibh praesent tristique magna sit amet purus. Tristique senectus et netus et malesuada fames ac turpis. Hac habitasse platea dictumst quisque. Auctor neque vitae tempus quam pellentesque nec nam aliquam. Ultrices tincidunt arcu non sodales neque sodales ut etiam. Iaculis at erat pellentesque adipiscing. Cras tincidunt lobortis feugiat vivamus. Nisi est sit amet facilisis magna etiam. Pharetra pharetra massa massa ultricies mi quis hendrerit. Vitae sapien pellentesque habitant morbi tristique senectus. Ornare aenean euismod elementum nisi quis eleifend quam adipiscing vitae."
  },
  {
    "objectID": "blog/third-post/index.html",
    "href": "blog/third-post/index.html",
    "title": "Third Blog Post",
    "section": "",
    "text": "The source for any page in your website could also be a Jupyter Notebook. This one is third-post/index.ipynb.\nHere’s an example I borrowed from the Seaborn docs:\n\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Plot the distribution of clarity ratings, conditional on carat\nsns.displot(\n    data=diamonds,\n    x=\"carat\", hue=\"cut\",\n    kind=\"kde\", height=4, aspect=1.5,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\",   \n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Caleb Vander Wall",
    "section": "",
    "text": "A little bit about me and my life."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Caleb Vander Wall",
    "section": "Education",
    "text": "Education\nUniversity of XYZ, City | Location | Sept 20XX - June 20XX"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Caleb Vander Wall",
    "section": "Experience",
    "text": "Experience\nWorkplace | Job title | April 20XX - present"
  }
]
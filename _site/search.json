[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting House Prices with Machine Learning\n\n\n\nPython\n\n\nMachine Learning\n\n\nData Cleaning\n\n\n\nThis project involves using machine learning algorithms to predict house prices based on various features such as location, size, and amenities. It includes data cleaning, feature engineering, and model selection.\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segmentation Using Clustering Techniques\n\n\n\nR\n\n\nMachine Learning\n\n\nClustering\n\n\nStatistical Modelling\n\n\n\nThis project focuses on segmenting customers into different groups based on their purchasing behavior and demographics. It uses clustering algorithms like K-means and hierarchical clustering to identify distinct customer segments.\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Global CO2 Emissions\n\n\n\nR\n\n\nData Visualization\n\n\nEnvironmental Science\n\n\n\nThis project involves creating visualizations to show trends in global CO2 emissions over time. It includes data extraction from public databases, data cleaning, and using visualization libraries to create interactive charts and graphs.\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Fnal Project.html",
    "href": "Fnal Project.html",
    "title": "Replace this with an interesting title",
    "section": "",
    "text": "Over the past 3 years I’ve become interested in fantasy football. This has lead me to explore football statistics in order to better predict the performance of players and teams. I’ve also grown up with two sisters who have been die hard Taylor Swift fans their whole lives. Though these two facts about my life seem to be unrelated however, If you’ve watched a Kansis city chiefs game in the past two years or heard any news at all about Taylor Swift, you would know that they are unfortunately very related. The world was set ablaze in September 2023 when Taylor Swift and Travis Kelce announced they were in a relationship. Ticket and Travis Kelce Jersey revenue skyrocketed as the market was introduced to millions of Swifties trying to secure the latest Taylor Swift clout so they could where it to her next show. As their high profile relationship continues, I am curious to see how this relationship would effect the Kansas City Chiefs Performance."
  },
  {
    "objectID": "Fnal Project.html#motivation-and-context",
    "href": "Fnal Project.html#motivation-and-context",
    "title": "Replace this with an interesting title",
    "section": "",
    "text": "Over the past 3 years I’ve become interested in fantasy football. This has lead me to explore football statistics in order to better predict the performance of players and teams. I’ve also grown up with two sisters who have been die hard Taylor Swift fans their whole lives. Though these two facts about my life seem to be unrelated however, If you’ve watched a Kansis city chiefs game in the past two years or heard any news at all about Taylor Swift, you would know that they are unfortunately very related. The world was set ablaze in September 2023 when Taylor Swift and Travis Kelce announced they were in a relationship. Ticket and Travis Kelce Jersey revenue skyrocketed as the market was introduced to millions of Swifties trying to secure the latest Taylor Swift clout so they could where it to her next show. As their high profile relationship continues, I am curious to see how this relationship would effect the Kansas City Chiefs Performance."
  },
  {
    "objectID": "Fnal Project.html#main-objective",
    "href": "Fnal Project.html#main-objective",
    "title": "Replace this with an interesting title",
    "section": "Main Objective",
    "text": "Main Objective\nHow does Taylor Swift’s attendance at Kansas City Chiefs games effect the offensive performance."
  },
  {
    "objectID": "Fnal Project.html#packages-used-in-this-analysis",
    "href": "Fnal Project.html#packages-used-in-this-analysis",
    "title": "Replace this with an interesting title",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nlibrary(tidyverse)\nlibrary(kknn)\nlibrary(Lahman)\nlibrary(probably)\nlibrary(knockoff)\nlibrary(selectiveInference)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(readr)\nlibrary(stringr)\nlibrary(ggplot2)  \nlibrary(dplyr)\n\n\n\n\nPackage\nUse\n\n\n\n\nhere\nto easily load and save data\n\n\nreadr\nto import the CSV file data\n\n\ndplyr\nto massage and summarize data\n\n\nrsample\nto split data into training and test sets\n\n\nggplot2\nto create nice-looking and informative graphs"
  },
  {
    "objectID": "Fnal Project.html#data-description",
    "href": "Fnal Project.html#data-description",
    "title": "Replace this with an interesting title",
    "section": "Data Description",
    "text": "Data Description\nThe data I will be using is Kansas City Chiefs season performance statistics form https://www.pro-football-reference.com/ from 2013 to 2024. This data is collected from the staff of Pro Football Reference. It is collected in order for fans to better understand th teams they love and support. This site is the leading football statistical reference site, and is sited by many others. I am constricting data to 2013 - 2024 since 2013 is when Patrick Mahomes was drafted onto the Kansas City Chiefs ans started a new era of their franchise. Some key variable sin this data is: Points Scored, Points Allowed, Total Yards, Total Passing Yards, Totaly Rushing Yards and Touchdowns.\n\nkcc_g_16 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_16.txt\", show_col_types = FALSE)\nkcc_g_24 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_24.txt\", show_col_types = FALSE)\nkcc_g_23 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_23.txt\", show_col_types = FALSE)\nkcc_g_22 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_22.txt\", show_col_types = FALSE)\nkcc_g_21 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_21.txt\", show_col_types = FALSE)\nkcc_g_20 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_20.txt\", show_col_types = FALSE)\nkcc_g_19 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_19.txt\", show_col_types = FALSE)\nkcc_g_18 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_18.txt\", show_col_types = FALSE)\nkcc_g_17 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_17.txt\", show_col_types = FALSE)\nkcc_g_16 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_16.txt\", show_col_types = FALSE)\nkcc_g_15 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_15.txt\", show_col_types = FALSE)\nkcc_g_14 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_14.txt\", show_col_types = FALSE)\nkcc_g_13 &lt;- read_csv(\"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data/kcc_g_13.txt\", show_col_types = FALSE)\n\n\nData Limitations\nWhen choosing a response variable based on this data set is a little tricky. The natural two to select are touchdowns and Points scored. Although using these variables as an offensive indicator of success is misleading since they do not take into account who the opposing team is. Scoring a massive amount of points against the New York giants (worst in league) may be less impressive than and average amount of points scored against the Detroit Lions (best in League). A better indicator of offensive success is how a team did reltive to their predicted score. This data does not have this predicted score data we need.\nThis data also does not indicate whether Taylor Swift attended the game or not. We will have to find this info and input it in the data in oour data wrangling phase.\nThe data frames also have unnamed colums and rows with there actual names being observations, We will need to convert the column and row names to the righ ones."
  },
  {
    "objectID": "Fnal Project.html#data-wrangling-optional-section",
    "href": "Fnal Project.html#data-wrangling-optional-section",
    "title": "Replace this with an interesting title",
    "section": "Data Wrangling (Optional Section)",
    "text": "Data Wrangling (Optional Section)\n\nfile_paths &lt;- list.files(path = \"C:/Users/caleb/OneDrive/Desktop/School/Math 437/Final Project/Project Data\", \n                         pattern = \"kcc_g_.*\\\\.txt$\", full.names = TRUE)\n\nnew_col_names &lt;- c(\"Week\", \"Day\", \"Date\", \"Time\", \"Boxscore\", \"Outcome\", \"OT\", \"Rec\", \n                   \"Location\", \"Ppp\", \"PtS\", \"PtA\", \"FirstD\", \"TotYd\", \"PassY\", \"RushY\", \n                   \"TO\", \"FirstD_Alwd\", \"TotYd_Alwd\", \"PassY_Alwd\", \"RushY_Alwd\", \n                   \"TO_Alwd\", \"ExOff\", \"ExDef\", \"ExSpec\")\n\nkcc_data &lt;- lapply(file_paths, function(file) {\n  df &lt;- read_csv(file, skip = 1, show_col_types = FALSE)   # Read the file\n  colnames(df) &lt;- new_col_names  # Rename columns\n  year_suffix &lt;- str_extract(basename(file), \"\\\\d{2}\")\n  full_year &lt;- as.integer(paste0(\"20\", year_suffix))  # Convert to 4-digit year\n\n  df &lt;- df |&gt; filter(Week != \"Playoffs\") #Removes playoff row\n  df &lt;- df |&gt; filter(Ppp != \"Bye Week\") #removes bie week row\n  df &lt;- df |&gt; mutate(Taylor_Attend = \"no\") #Adds attendance collumn\n  df &lt;- df |&gt;  mutate(Year = full_year) |&gt;  # Add Year column\n    \n    relocate(Year, .before = Week)  # Move Year column before Week\n  df &lt;- df |&gt; mutate(\n    Date = paste(Date, full_year),  # Combine Date and Year\n    Date = as.Date(Date, format = \"% %d %Y\")  # Convert to Date format (e.g., \"September 13 2013\")\n  )\n\n  df &lt;- df |&gt; mutate(\n    Week = case_when(  # Replace playoff names with sequential week numbers\n        Week == \"Wild Card\" ~ \"18\",\n        Week == \"Division\" ~ \"19\",\n        Week == \"Conf. Champ.\" ~ \"20\",\n        Week == \"SuperBowl\" ~ \"21\",\n        TRUE ~ as.character(Week)  # Keep regular weeks unchanged\n      )\n  )\n  return(df)  # Return the modified dataframe\n})\n\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...9`\n• `Opp` -&gt; `Opp...10`\n• `Opp` -&gt; `Opp...12`\n• `1stD` -&gt; `1stD...13`\n• `TotYd` -&gt; `TotYd...14`\n• `PassY` -&gt; `PassY...15`\n• `RushY` -&gt; `RushY...16`\n• `TO` -&gt; `TO...17`\n• `1stD` -&gt; `1stD...18`\n• `TotYd` -&gt; `TotYd...19`\n• `PassY` -&gt; `PassY...20`\n• `RushY` -&gt; `RushY...21`\n• `TO` -&gt; `TO...22`\n\nnames(kcc_data) &lt;- gsub(\".*/|\\\\.txt$\", \"\", file_paths)  # Remove path and .txt extension\n\n# Assign each dataframe to a separate variable\nfor (name in names(kcc_data)) {\n  assign(name, kcc_data[[name]])\n}\n\nTaylor_Attend23 &lt;- c(\"no\", \"no\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\")\nTaylor_Attend24 &lt;- c(\"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\")\n\nkcc_g_23 &lt;- kcc_g_23 |&gt; mutate(Taylor_Attend = Taylor_Attend23)\nkcc_g_24 &lt;- kcc_g_24 |&gt; mutate(Taylor_Attend = Taylor_Attend24)"
  },
  {
    "objectID": "Fnal Project.html#exploratory-data-analysis",
    "href": "Fnal Project.html#exploratory-data-analysis",
    "title": "Replace this with an interesting title",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWe will split the data inot a training and test set. We will train the data with 2013 - 2022 seasons. This is because 2013 marks the year Patrick MMMahomes was drafted onto the chiefs and the start of the dramatic improvement of Kansas City over the next 10 years. We end the training data at 2022 since Travis and Taylor’s relationship began in 2023. We will also split the data into the years of the Chiefs dynasty 2019 - 2024.\n\nyears_to_include &lt;- list(kcc_g_13, kcc_g_14, kcc_g_15, kcc_g_16, kcc_g_17, kcc_g_18, kcc_g_19, kcc_g_20, kcc_g_21, kcc_g_22)\n\nkcc_g_train &lt;- bind_rows(years_to_include)\n\nkcc_g_test &lt;- bind_rows(kcc_g_23, kcc_g_24)\n\nkcc_g_all &lt;- bind_rows(kcc_g_train, kcc_g_test)\n\nkcc_g_dyn &lt;- bind_rows(kcc_g_19, kcc_g_20, kcc_g_21, kcc_g_22, kcc_g_23, kcc_g_24)\n\nkcc_g_taylor &lt;- filter(kcc_g_test, Taylor_Attend == \"yes\")\n\nkcc_g_taylorless &lt;- filter(kcc_g_test, Taylor_Attend == \"no\")\n\nWe are choosing our response variable to be points scored instead of touchdowns. Points scored is a better indicator of success for an offense because it includes field goals. Field goals show a minor success in an offense. Here is the distribution of points scored per game from 2013 - 2024 seasons\n\nggplot(kcc_g_all, aes(x = as.factor(Year), y = PtS, fill = as.factor(Year))) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Points Scored Per Game Distribution by Year\",\n       x = \"Year\", \n       y = \"Points Scored Per Game (PtS)\")\n\n\n\n\n\n\n\n\nThe following is a box plot displaying the distributions of points scored by the Chiefs when Taylor was attending verses when she was not attending from 2013 - 2024\n\nkcc_g_all |&gt; group_by(Taylor_Attend) |&gt; \n  summarize(\n    num_total = n(),\n    num_missing = sum(is.na(PtS)),\n    min = min(PtS, na.rm = TRUE),\n    Q1 = quantile(PtS, 0.25, na.rm = TRUE),\n    median = median(PtS, na.rm = TRUE),\n    Q3 = quantile(PtS, 0.75, na.rm = TRUE),\n    max = max(PtS, na.rm = TRUE),\n    mean = mean(PtS, na.rm = TRUE),\n    sd = sd(PtS, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 10\n  Taylor_Attend num_total num_missing   min    Q1 median    Q3   max  mean    sd\n  &lt;chr&gt;             &lt;int&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 no                  199           0     0    21     27    33    56  27.1  9.53\n2 yes                  23           0    14    19     25    27    41  24.3  6.14\n\nggplot(data = kcc_g_all, aes(x = Taylor_Attend, y = PtS, fill = Taylor_Attend)) + \n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Avoids overlapping points\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Boxplot of PtS 2013 - 2024\",\n       x = \"Taylor Attend\",\n       y = \"PtS\") +\n  scale_fill_manual(values = c( \"no\" = \"red\", \"yes\" = \"blue\"), labels = c(\"Games Taylor Did Not Attend\", \"Games Taylor Attended\"))\n\n\n\n\n\n\n\n\nAs you can see, the games Taylor attended seems to have a lower distribution, with a slightly smaller mean. However this change in distribution does not seem to be very significant.\nLets try narrowing the distribution of games Taylor did not attend to 2019 - 2024 (the start to the Chiefs dynasty).\n\nkcc_g_test |&gt; group_by(Taylor_Attend) |&gt; \n  summarize(\n    num_total = n(),\n    num_missing = sum(is.na(PtS)),\n    min = min(PtS, na.rm = TRUE),\n    Q1 = quantile(PtS, 0.25, na.rm = TRUE),\n    median = median(PtS, na.rm = TRUE),\n    Q3 = quantile(PtS, 0.75, na.rm = TRUE),\n    max = max(PtS, na.rm = TRUE),\n    mean = mean(PtS, na.rm = TRUE),\n    sd = sd(PtS, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 10\n  Taylor_Attend num_total num_missing   min    Q1 median    Q3   max  mean    sd\n  &lt;chr&gt;             &lt;int&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 no                   18           0     0    17     21    27    31  20.5  7.91\n2 yes                  23           0    14    19     25    27    41  24.3  6.14\n\nggplot(data = kcc_g_test, aes(x = Taylor_Attend, y = PtS, fill = Taylor_Attend)) + \n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Avoids overlapping points\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Boxplot of PtS during 2023 - 2024\",\n       x = \"Taylor Attend\",\n       y = \"PtS\") +\n  scale_fill_manual(values = c( \"no\" = \"red\", \"yes\" = \"blue\"), labels = c(\"Games Taylor Did Not Attend\", \"Games Taylor Attended\"))\n\n\n\n\n\n\n\n\nHere we can see that the games Taylor attended have a greater distribution then those she did not attend.\n\nlibrary(ggplot2)\n\nggplot(kcc_g_test, aes(x = TotYd, y = PtS, color = Taylor_Attend)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(\n    title = \"Chiefs Points Scored vs Total Yards\",\n    x = \"Total Yards (TotYd)\",\n    y = \"Points Scored (PtS)\",\n    color = \"Taylor Swift Attendance\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(kcc_g_test, aes(x = ExOff, y = PtS, color = Taylor_Attend)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(\n    title = \"Chiefs ExOff vs Total Yards\",\n    x = \"Total Yards (TotYd)\",\n    y = \"Points Scored (PtS)\",\n    color = \"Taylor Swift Attendance\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(kcc_g_test, aes(x = FirstD, y = PtS, color = Taylor_Attend)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(\n    title = \"Chiefs First Down Count vs Total Yards\",\n    x = \"Total Yards (TotYd)\",\n    y = \"Points Scored (PtS)\",\n    color = \"Taylor Swift Attendance\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe will now find appropriate explanitory variables with LASSO\n\nkcc_lasso_model &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 1) # mixture = 1 specifies pure LASSO\n\nkcc_lasso_recipe &lt;- recipe(\n  PtS ~ PtA \n  + FirstD \n  + TotYd \n  + PassY \n  + RushY \n  + TotYd_Alwd \n  + PassY_Alwd \n  + RushY_Alwd, # response ~ predictors\n  data = kcc_g_train\n) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nkcc_lasso_wflow &lt;- workflow() |&gt;\n  add_model(kcc_lasso_model) |&gt;\n  add_recipe(kcc_lasso_recipe) # same recipe is needed, no need to reinvent the wheel\n\n\nkcc_cv &lt;- vfold_cv(kcc_g_train, v = 10)\nkcc_lasso_tune &lt;- tune_grid(kcc_lasso_model, \n                      kcc_lasso_recipe, \n                      resamples = kcc_cv, \n                      grid = grid_regular(penalty(range = c(-2, 2)), levels = 50))\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\n\n\nkcc_lasso_tune |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nlasso_best &lt;- kcc_lasso_tune |&gt;\n  select_best(\n    metric = \"rmse\"\n    #,desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nlasso_best\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.356 Preprocessor1_Model20\n\n\n\nlibrary(modeltime)\nlibrary(ggplot2)\nlasso_wflow_final &lt;- kcc_lasso_wflow |&gt;\n  finalize_workflow(parameters = lasso_best) \n\nlasso_pred_check &lt;- lasso_wflow_final |&gt;\n  fit_resamples(\n    resamples = kcc_cv,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\ncal_plot_regression(\n  lasso_pred_check,\n  truth = PtS,\n  estimate = .pred\n)\n\n\n\n\n\n\n\n\n\nkcc_lasso_fit &lt;- lasso_wflow_final |&gt;\n  fit(data = kcc_g_train)\nkcc_lasso_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00 4.9620\n2   1  4.72 4.5210\n3   1  8.63 4.1190\n4   1 11.88 3.7530\n5   1 14.58 3.4200\n6   1 16.82 3.1160\n7   1 18.68 2.8390\n8   1 20.22 2.5870\n9   1 21.51 2.3570\n10  1 22.57 2.1480\n11  1 23.45 1.9570\n12  1 24.19 1.7830\n13  2 25.10 1.6250\n14  3 26.07 1.4800\n15  3 26.88 1.3490\n16  4 27.56 1.2290\n17  4 28.27 1.1200\n18  4 28.87 1.0200\n19  4 29.36 0.9297\n20  4 29.76 0.8471\n21  4 30.10 0.7719\n22  4 30.38 0.7033\n23  4 30.62 0.6408\n24  4 30.81 0.5839\n25  4 30.97 0.5320\n26  4 31.10 0.4848\n27  4 31.21 0.4417\n28  4 31.31 0.4025\n29  4 31.38 0.3667\n30  4 31.45 0.3341\n31  4 31.50 0.3044\n32  4 31.54 0.2774\n33  4 31.58 0.2528\n34  4 31.61 0.2303\n35  4 31.63 0.2098\n36  4 31.65 0.1912\n37  4 31.67 0.1742\n38  4 31.69 0.1587\n39  5 31.71 0.1446\n40  6 31.76 0.1318\n41  6 31.80 0.1201\n42  6 31.83 0.1094\n43  6 31.86 0.0997\n44  6 31.88 0.0908\n45  6 31.90 0.0828\n46  6 31.92 0.0754\n\n...\nand 22 more lines.\n\n\n\nkcc_lasso_fit |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nkcc_lasso_coef &lt;- kcc_lasso_fit |&gt;\n  broom::tidy()\nkcc_lasso_coef \n\n# A tibble: 9 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   27.8     0.356\n2 PtA            0       0.356\n3 FirstD         0.674   0.356\n4 TotYd          3.70    0.356\n5 PassY          0       0.356\n6 RushY          0.683   0.356\n7 TotYd_Alwd     0       0.356\n8 PassY_Alwd     1.29    0.356\n9 RushY_Alwd     0       0.356\n\n\nHere we can see that important variables are, First Down Count, Total Yards, Rushing Yards, and Passing Yards Allowed. We will now train a model with this data."
  },
  {
    "objectID": "Fnal Project.html#modeling",
    "href": "Fnal Project.html#modeling",
    "title": "Replace this with an interesting title",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "Fnal Project.html#insights",
    "href": "Fnal Project.html#insights",
    "title": "Replace this with an interesting title",
    "section": "Insights",
    "text": "Insights\n\nLimitations and Future Work\n\n\nReflection (Optional Subsection)"
  },
  {
    "objectID": "blog/second-post/index.html",
    "href": "blog/second-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Amet cursus sit amet dictum sit amet. Eget duis at tellus at urna condimentum. Convallis aenean et tortor at risus viverra. Tincidunt ornare massa eget egestas purus viverra accumsan. Et malesuada fames ac turpis egestas. At imperdiet dui accumsan sit amet. Ut ornare lectus sit amet est placerat. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Duis ultricies lacus sed turpis tincidunt id aliquet risus. Mattis enim ut tellus elementum sagittis. Dui id ornare arcu odio ut. Natoque penatibus et magnis dis. Libero justo laoreet sit amet cursus sit. Sed faucibus turpis in eu. Tempus iaculis urna id volutpat lacus laoreet.\nPhasellus vestibulum lorem sed risus. Eget felis eget nunc lobortis mattis. Sit amet aliquam id diam maecenas ultricies. Egestas maecenas pharetra convallis posuere morbi. Etiam erat velit scelerisque in dictum non consectetur a erat. Cras fermentum odio eu feugiat pretium nibh ipsum consequat. Viverra accumsan in nisl nisi scelerisque. Et netus et malesuada fames ac. Amet tellus cras adipiscing enim eu turpis egestas pretium aenean. Eget lorem dolor sed viverra ipsum nunc aliquet. Ultrices dui sapien eget mi proin sed libero enim sed. Ultricies mi eget mauris pharetra et ultrices neque. Ipsum suspendisse ultrices gravida dictum. A arcu cursus vitae congue mauris rhoncus aenean vel. Gravida arcu ac tortor dignissim convallis. Nulla posuere sollicitudin aliquam ultrices."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/first-post/index.html",
    "href": "blog/first-post/index.html",
    "title": "First Post",
    "section": "",
    "text": "Sed risus ultricies tristique nulla aliquet. Neque volutpat ac tincidunt vitae semper quis lectus nulla.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Enim sed faucibus turpis in eu mi bibendum neque. Ac orci phasellus egestas tellus rutrum tellus pellentesque eu. Velit sed ullamcorper morbi tincidunt ornare massa. Sagittis id consectetur purus ut faucibus pulvinar elementum integer. Tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Lobortis feugiat vivamus at augue eget arcu. Aliquam ut porttitor leo a diam sollicitudin tempor id eu. Mauris a diam maecenas sed enim ut sem viverra aliquet. Enim ut tellus elementum sagittis vitae et leo duis. Molestie at elementum eu facilisis sed odio morbi quis commodo. Sapien pellentesque habitant morbi tristique senectus. Quam vulputate dignissim suspendisse in est. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.\nVelit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Viverra mauris in aliquam sem fringilla ut morbi tincidunt augue. Tortor at auctor urna nunc id. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Aliquet nibh praesent tristique magna sit amet purus. Tristique senectus et netus et malesuada fames ac turpis. Hac habitasse platea dictumst quisque. Auctor neque vitae tempus quam pellentesque nec nam aliquam. Ultrices tincidunt arcu non sodales neque sodales ut etiam. Iaculis at erat pellentesque adipiscing. Cras tincidunt lobortis feugiat vivamus. Nisi est sit amet facilisis magna etiam. Pharetra pharetra massa massa ultricies mi quis hendrerit. Vitae sapien pellentesque habitant morbi tristique senectus. Ornare aenean euismod elementum nisi quis eleifend quam adipiscing vitae."
  },
  {
    "objectID": "blog/third-post/index.html",
    "href": "blog/third-post/index.html",
    "title": "Third Blog Post",
    "section": "",
    "text": "The source for any page in your website could also be a Jupyter Notebook. This one is third-post/index.ipynb.\nHere’s an example I borrowed from the Seaborn docs:\n\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Plot the distribution of clarity ratings, conditional on carat\nsns.displot(\n    data=diamonds,\n    x=\"carat\", hue=\"cut\",\n    kind=\"kde\", height=4, aspect=1.5,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\",   \n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Caleb Vander Wall",
    "section": "",
    "text": "A little bit about me and my life."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Caleb Vander Wall",
    "section": "Education",
    "text": "Education\nUniversity of XYZ, City | Location | Sept 20XX - June 20XX"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Caleb Vander Wall",
    "section": "Experience",
    "text": "Experience\nWorkplace | Job title | April 20XX - present"
  }
]